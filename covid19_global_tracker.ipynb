{"nbformat": 4, "nbformat_minor": 5, "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python", "version": "3"}}, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# COVID-19 Global Data Tracker \u2014 Analysis with Pandas & Matplotlib\n", "\n", "This notebook demonstrates data loading, exploration, basic analysis, and visualizations using **pandas** and **matplotlib**.\n", "\n", "It follows the assignment requirements:\n", "- **Task 1:** Load & Explore (head, dtypes, missing values, cleaning)\n", "- **Task 2:** Basic Analysis (describe + groupby)\n", "- **Task 3:** Visualizations (line, bar, histogram, scatter)\n", "\n", "> Set `CSV_PATH` to your CSV file; otherwise, it falls back to the Iris dataset (or a synthetic iris-like dataset if `sklearn` is unavailable)."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Imports\n", "import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from pathlib import Path\n", "from IPython.display import display\n", "pd.set_option('display.max_columns', None)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Task 1 \u2014 Load & Explore the Dataset\n", "Set `CSV_PATH` to your CSV file (e.g., a COVID-19 timeseries). If `CSV_PATH` is `None`, we will load **Iris** from scikit-learn, or generate a small synthetic dataset if needed."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["CSV_PATH = None  # e.g., Path('data/owid.csv')\n", "\n", "def load_dataset(csv_path=None):\n", "    if csv_path:\n", "        try:\n", "            df = pd.read_csv(csv_path)\n", "            return df, f\"CSV: {csv_path}\"\n", "        except FileNotFoundError:\n", "            print(\"File not found; falling back to iris dataset.\")\n", "        except pd.errors.EmptyDataError:\n", "            print(\"Empty CSV; falling back to iris dataset.\")\n", "        except Exception as e:\n", "            print(f\"Error reading CSV ({e}); falling back to iris dataset.\")\n", "\n", "    # Try Iris from sklearn\n", "    try:\n", "        from sklearn.datasets import load_iris\n", "        iris = load_iris(as_frame=True)\n", "        df = iris.frame.copy()\n", "        if 'target' in df.columns:\n", "            df['species'] = df['target'].map(dict(enumerate(iris.target_names)))\n", "            df.drop(columns=['target'], inplace=True)\n", "        return df, 'iris (sklearn)'\n", "    except Exception:\n", "        pass\n", "\n", "    # Synthetic iris-like fallback\n", "    rng = np.random.default_rng(42)\n", "    n = 150\n", "    df = pd.DataFrame({\n", "        'sepal length (cm)': rng.normal(5.8, 0.8, n),\n", "        'sepal width (cm)':  rng.normal(3.0, 0.4, n),\n", "        'petal length (cm)': rng.normal(3.7, 1.5, n),\n", "        'petal width (cm)':  rng.normal(1.1, 0.5, n),\n", "        'species': rng.choice(['setosa','versicolor','virginica'], n)\n", "    })\n", "    return df, 'synthetic iris-like'\n", "\n", "df, DATA_SOURCE = load_dataset(CSV_PATH)\n", "print('Data source:', DATA_SOURCE)\n", "display(df.head())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Explore structure and missing values"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["display(df.dtypes)\n", "display(df.isna().sum())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Clean the dataset\n", "Fill numeric missing values with **median** and categorical with **mode**. Drop rows that are completely NA."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["df_clean = df.copy()\n", "num_cols = df_clean.select_dtypes(include=[np.number]).columns\n", "cat_cols = df_clean.select_dtypes(include=['object','category']).columns\n", "\n", "for c in num_cols:\n", "    if df_clean[c].isna().any():\n", "        df_clean[c] = df_clean[c].fillna(df_clean[c].median())\n", "\n", "for c in cat_cols:\n", "    if df_clean[c].isna().any():\n", "        mode = df_clean[c].mode(dropna=True)\n", "        fill = mode.iloc[0] if not mode.empty else 'Unknown'\n", "        df_clean[c] = df_clean[c].fillna(fill)\n", "\n", "df_clean = df_clean.dropna(how='all')\n", "display(df_clean.head())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Task 2 \u2014 Basic Data Analysis\n", "Compute descriptive statistics and a grouped mean over a categorical column."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Descriptive statistics for numeric columns\n", "display(df_clean.describe(numeric_only=True))\n", "\n", "# Choose a categorical column (prefer 'species'); otherwise pick any object/category column,\n", "# or create bins from the first numeric column\n", "cat_col = 'species' if 'species' in df_clean.columns else None\n", "if cat_col is None:\n", "    for c in df_clean.columns:\n", "        if pd.api.types.is_categorical_dtype(df_clean[c]) or pd.api.types.is_object_dtype(df_clean[c]):\n", "            cat_col = c\n", "            break\n", "if cat_col is None:\n", "    first_num = df_clean.select_dtypes(include=[np.number]).columns[0]\n", "    cat_col = f\"{first_num}_bin\"\n", "    df_clean[cat_col] = pd.qcut(df_clean[first_num], q=3, labels=['low','mid','high'])\n", "\n", "num_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n", "num_a = num_cols[0]\n", "num_b = num_cols[1] if len(num_cols) > 1 else num_cols[0]\n", "\n", "group_means = df_clean.groupby(cat_col)[num_a].mean().sort_values(ascending=False)\n", "display(group_means)\n", "print(f'Grouping column: {cat_col} | Numeric A: {num_a} | Numeric B: {num_b}')\n", "print(f'Top group by mean {num_a}: {group_means.index[0]} -> {group_means.iloc[0]:.3f}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Task 3 \u2014 Data Visualization (Matplotlib)\n", "Four plots: **line**, **bar**, **histogram**, **scatter**.\n", "> Per instructions, we use matplotlib only; you may add seaborn styling if you like."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def ensure_time_index(df):\n", "    for c in df.columns:\n", "        if pd.api.types.is_datetime64_any_dtype(df[c]):\n", "            return pd.to_datetime(df[c])\n", "    return pd.date_range(start='2020-01-01', periods=len(df), freq='D')\n", "\n", "dates = ensure_time_index(df_clean)\n", "series = pd.Series(df_clean[num_a].values, index=dates).rolling(window=7, min_periods=1).mean()\n", "\n", "plt.figure()\n", "series.plot()\n", "plt.title(f'Trend of {num_a} over time (7-day rolling mean)')\n", "plt.xlabel('Date')\n", "plt.ylabel(num_a)\n", "plt.tight_layout()\n", "plt.show()"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Bar chart: mean of num_a by category\n", "plt.figure()\n", "df_clean.groupby(cat_col)[num_a].mean().sort_values().plot(kind='bar')\n", "plt.title(f'Average {num_a} by {cat_col}')\n", "plt.xlabel(cat_col)\n", "plt.ylabel(f'Mean {num_a}')\n", "plt.tight_layout()\n", "plt.show()"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Histogram: distribution of num_a\n", "plt.figure()\n", "plt.hist(df_clean[num_a].values, bins=20)\n", "plt.title(f'Distribution of {num_a}')\n", "plt.xlabel(num_a)\n", "plt.ylabel('Count')\n", "plt.tight_layout()\n", "plt.show()"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Scatter: num_a vs num_b\n", "plt.figure()\n", "plt.scatter(df_clean[num_a].values, df_clean[num_b].values)\n", "plt.title(f'{num_a} vs {num_b}')\n", "plt.xlabel(num_a)\n", "plt.ylabel(num_b)\n", "plt.tight_layout()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Findings / Observations\n", "- Add 2\u20134 short bullet points with your observations.\n", "- Example: The 7-day rolling mean shows clear waves.\n", "- Example: One group has the highest average of the selected metric."]}]}